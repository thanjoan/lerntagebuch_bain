---
title: "Tag 8 - Nachtrag zu Metadaten und Schnittstellen // Suchmaschinen und Discovery-Systeme, Teil 1"
date: 2020-11-27
---

Tag 8 begann mit einem Nachtrag zum Thema Metadaten modellieren, bevor wir dann ins Thema Suchmaschinen und Discovery-Systeme einstiegen und uns mit VuFind und Solr befassten.


## Nachtrag Metadaten und Schnittstellen
Zunächst ein Input zur sehr wichtigen Validierung von Daten. Anschliessend folgen weitere Tools zur Metadatentransormation, ein Ausflug in die Nutzung von JSON-APIs, ein Exkurs zu ScrAPIr und zu LIDO.


### Validierung von XML
Das Validieren ist ein sehr wichtiger Schritt. Darum soll hier festgehalten werden, wie das gemacht werden kann. Zum Validieren der XML-Daten werden diese mit einem Schema abgeglichen. Dieses Schema wird auf der [Webseite der Library of Congress](http://www.loc.gov/standards/marcxml/) zur Verfügung gestellt:

![Screenshot Webseite Library of Congress](https://pad.gwdg.de/uploads/upload_df5eccd1afebd4b2add1e34f7ae2708a.png)

Mit so einem Schema werden Regeln festgelegt, dabei werden sowohl Form wie Inhalte festgelegt und es kann entsprechend beides validiert werden.

Das Vorgehen:

1. Die Daten aus dem OpenRefine als XML exportieren; dazu als heruntergeladene Datei die Dateiendung in .xml ändern, weil es aus OpenRefine heraus als .txt daherkommt.
2. Das Schema herunterladen, in den entsprechenden Ordner, darum lautet der erste Befehl `cd Downloads` (wenn es im Downloadordner zu liegen kommen soll).
3. Mit dem Programm `xmllint` validieren (dieses ist unter Ubuntu bereits vorinstalliert).

Für die Schritte zwei und drei werden die folgenden Befehle eingegeben. Besser statt `*xml` zu schreiben, ist es den konkreten Dateinamen, welchen man validiern möchte, einzugeben, weil `*xml` geht über alle xml-Dateien.

![Code zum validieren](https://pad.gwdg.de/uploads/upload_a7c4663a7b042b22b0d8419bd2b6fbf9.png)

<small>Quelle: [Das gemeinsame Codi](https://pad.gwdg.de/ywogyRNTQ_CTg9PvrQywsQ?view).</small>

Anschliessend zeigt es im Terminal die Fehler an, im Abgleich mit der geöffneten Datei im Texteditor lassen sich die Fehler dann eruieren.
Auf folgendem Bild ist zu sehen, wie dies im Terminal aussieht. Zu sehen sind alle eingegebenen Befehle und dass die Daten alle validiert, d.h. frei von Fehlern sind:

![Screenshot Terminal Validierung](https://pad.gwdg.de/uploads/upload_5137cf49a403d9b3fc724845cca0d205.png)


### Weitere Tools zur Metadatentransformation
Neben OpenRefine gibt es noch andere Tools. OpenRefine hat dabei einige Vorteile, wie beispielsweise die grafische Oberfläche, was die Arbeit damit erleichtert. Zudem eignet sich OpenRefine sehr gut für die Datenanreicherung (Reconciliation).

Die Wahl der Software hängt ab vom Metadatenformat und was damit gemacht werden soll. Ein weiteres Kriterium, gerade in der Praxis, ist die Programmiersprache. Denn da eine grafische Oberfläche nicht immer möglich ist, ist es am effizientesten mit einem Tool zu arbeiten, wo man die Sprache kennt.

So eignet sich für Pearl [Catmandu](https://librecat.org/) und für Java [Metafacture](https://github.com/metafacture/metafacture-core). Für MARC21 eignet sich, wie bereits im Kurs gesehen, [MarcEdit](https://marcedit.reeset.net/). Für Python gibt es nicht eine vollumfängliche Lösung, aber für MARC21 Daten gibt es z.B. [pymarc](https://pymarc.readthedocs.io/en/latest/).


### Nutzung von JSON-APIs
JSON-APIs? Sieben Buchstaben mit Bindestrich. Mehr stand da für mich erstmal nicht, ausser dass ich beide Buchstabenteile schonmal gehört hatte, sie allerdings in den Untiefen meines Gedächtnisses nicht mehr auffinden oder gar sinnvoll kombinieren konnte. Eine Auffrischung ist an dieser Stelle also wohl dringend nötig.

[JSON](https://www.json.org/json-de.html) steht für JavaScript Object Notation und ist ein Format für den Datenaustausch, basiert auf JavaScript und kann beispielsweise anstelle von XML verwendet werden (siehe [wiki.selfhtml](https://wiki.selfhtml.org/wiki/JSON)).

[API](https://de.wikipedia.org/wiki/Programmierschnittstelle) steht für *application programming interface*. Es ist also eine Schnittstelle zur Anbindung von Programmen, die auf Quelltextebene dafür sorgt, dass die Daten ausgetauscht werden können.

Die Schnittstellen, welche wir hier im Kurs bislang betrachtet haben, sind Archiv- und Bibliotheksspezifisch und dort auch immer noch zentral. Ausserhalb dieser Bereiche, werden in der Regel Schnittstellen verwendet, die JSON ausgeben, denn JSON ist das am meisten verwendete Format für moderne Web-Schnittstellen. Deswegen ist es wichtig, diese JSON-APIs auch zu kennen.

Ein Beispiel für eine JSON-API ist [lobid-gnd](https://lobid.org/gnd/api). Hier sieht man gut, was JSON ausmacht: Es lässt sich (wie xml) recht leserlich im Browser anzeigen und eignet sich zugleich gut für die maschinelle Verarbeitung.


### Exkurs: ScrAPIr
[ScrAPIr](https://scrapir.org/) ist ein Tool, womit Web-Schnittstellen auch für Nicht-Informatiker\*innen einfache zugänglich sein sollen. Es können hier Daten über die Web-APIs geholt werden. So können Webseiten wie Google Books, The New York Times oder auch NASA Images abgefragt werden.

Die Benutzerschnittstelle ist in Form einer Tabelle dargestell und lässt sich recht einfach abfragen. Die Ergebnisse kann man sich dann in verschiedenen Formaten (CSV, JSON, Code) ausgeben und/oder speichern. Der Code lässt sich entweder in Python oder JavaScript anzeigen. Das ist sehr nützlich, weil man dadurch sieht, wie die Schnittstelle in Form von Code abgefragt werden könnte. Zudem lässt sich die Tabellenansicht verändern: Tabellenasicht, Ansicht in Form einer Baumstruktur oder in Form einer Liste. Die letztere Ansicht ist besonders leicht zu lesen und zeigt im Falle von NASA Images auch gleich das Bild mit an.

![Screenshot ScrAPIr](https://pad.gwdg.de/uploads/upload_2c181b2367766ff28855d04dc76ddfb6.png)

<small>Ansicht in ScrAPIr in der Listenansicht, zu sehen ist ein Eintrag aus NASA Images.</small>

### Exkurs: Metadatenstandard LIDO
[LIDO](https://de.wikipedia.org/wiki/Lightweight_Information_Describing_Objects) (Lightweight Information Describing Objects) ist ein XML-Standard, der auf dem [CIDOC Conceptual Reference Model](http://www.cidoc-crm.org/) basiert, das ist eine Ontologie für den Bereich des Kulturerbes und ein Modell, das dann für Standards verwendet wird, zum Informationsaustausch in Museen, Archiven oder auch Bibliotheken. Entsprechend wird LIDO vorallem in Museen eingesetzt.

Was LIDO speziell macht ist, dass es dem Linked-Open-Data-Konzept folgt, wodurch Querverweise zu anderen Objekten und auch Institutionen gelegt werden können. Zudem ist LIDO Ereigniszentriert und nicht Dokumentzentriert wie andere Standards (z.B. MARC), das bedeutet, es wird nun nicht vom Objekt ausgegangen, sondern es gibt verschiedene Ereignisse: wann wurde ein Werk geschaffen, wann wurde es verkauft, durch wen wurde es gekauft, wann entstand ein Schaden am Werk, wann wurde das Werk restauriert, usw. – das sind alles Ereignisse, die für ein Objekt und den Museumskontext wichtig sind und die nun in LIDO beschrieben werden. Festgehalten werden Personen, zeitliche Daten usw., dadurch können Beziehungen hergestellt werden. Das macht LIDO sehr interessant. Nachteilig hingegen könnte sein, dass durch diesen grundlegend anderen konzeptuellen Aufbau mit der Ereigniszentriertheit es recht kompliziert ist, einen Datenaustausch zwischen LIDO und dokumentzentrierten Formaten (verlustfrei) hinzubekommen.
