---
title: "Tag 6 - Metadaten modellieren und Schnittstellen nutzen, Teil 1"
date: 2020-10-30
---

Metadaten und Schnittstellen sind ein sehr wichtiges Thema – wenn nicht sogar das Kernthema des Kurses. Am heutigen Tag 6 befassen wir uns in erster Linie mit Harvesting und Crosswalks, d.h. also mit der Übertragung der Daten und den Schnittstellen.

Mit dem Modellieren werden wir uns in [Teil 2](https://thanjoan.github.io/lerntagebuch_bain/2020/11/20/tag-7.html) (an Tag 7) auseinandersetzen.


## Schnittstellen
Es gibt viele Schnittstellen (Übertragungsprotokolle), den wichtigsten und am weitesten verbreiteten Übertragungsprotokollen im Zusammenhang mit Bibliotheks- und Archivsystemen sind wir bereits mehrfach begegnet im Laufe des Semesters:

- **Z39.50** (von der Library of Congress standardisiert, stammt aus den 80er Jahren, wird aber immer noch verwendet; braucht eine spezielle Software für Anfragen)
- **SRU – Search/Retrieve via URL** (eine modernere Variante, funktioniert ähnlich wie Z39.50, ebenfalls von der Library of Congress; Anfragen via URL im Browser möglich)
- **OAI-PMH Open Archive Initiative Protocol for Metadata Harvesting** (von Open Archives Initiative erarbeitet, Library of Congress ist jedoch auch mitwirkend; Anfragen via URL im Browser möglich)


## Harvesting und Crosswalks
Anhand des Schaubildes lässt sich gut nachvollziehen, was wir bisher im Kurs gemacht haben, wo wir uns momentan befinden und was schliesslich das Ziel sein wird. (Anmerkung: Das Schaubild hat sich seit Kursbeginn etwas gewandelt, wir nutzen nun statt metha den OAI-Harvester VuFindHarvest.)

![Schaubild zum Kurs](https://pad.gwdg.de/uploads/upload_7e00c288f56ba6de0880e9cfc64b2c0d.png)

Wir haben je ein Bibliothekssystem (koha), ein Archivsystem (ArchivesSpace) und ein Repository (DSpace) kennengelernt und darin jeweils Datensätze erstellt. 

Diese Daten sollen nun aus den drei Systemen mit VuFindHarvest via die Schnittstellle OAI-PMH geholt (also geharvestet, «geerntet») werden, und in für die Systeme typischen Daten geladen werden:
- Koha: MARC21-XML
- ArchivesSpace: EAD
- DSpace: Dublin Core[^1]

Das Harvesting kann man sich wirklich bildhaft vorstellen, wie ein abgrasen oder auch ernten der Daten: Der Harvester grast über das System und holt sich die Daten, man spricht daher auch von einer Maschinenschnittstelle. Bei ArchivesSpace beispielsweise, greifen wir nicht wie bis anhin via localhost:8080 darauf zu, sondern nun wird ArchivesSpace nur gestartet und im Hintergrund öffnet ArchivesSpace einen speziellen Dienst für die OAI-PMH-Schnittstelle und den Harvester unter localhost:8082.

Da nun also drei verschiedene Formate herauskommen, müssen diese als nächstes in ein einheitliches Format gebracht werden: in MARC21-XML. Dazu benötigen wir ein weiteres Tool, marcEdit, mit welchem wir einen XSLT-Crosswalk machen, wodurch dann die Daten alle einheitlich in MARC21-XML vorliegen.

Mit diesen Daten werden wir dann in der Vorlesung von [Tag 9](https://thanjoan.github.io/lerntagebuch_bain/2020/12/11/tag-9.html) weiterarbeiten, und zwar mit dem Suchindex [Solr](https://lucene.apache.org/solr/); dies ist eine Volltext-Suchmaschine und ist Teil von VuFind, einem Discovery-System, und ist Open Source. Nebst Solr gibt es noch [Elasticsearch](https://www.elastic.co/de/what-is/elasticsearch), welche auch Open Source ist. Amazon, Netflix und viele mehr nutzen eine dieser beiden Volltext-Suchmaschinen im Hintergrund.)

[^1]: Dublin Core ist nicht das beste Format im Zusammenhang mit DSpace, aber hier für unsere Übung geeignet, weil Dublin Core ein weit verbreitetes Format ist, das wir kennen sollten.

Nun geht es daran dies alles umzusetzen und in der Anwendung zu erfahren. Wir haben nun also folgende zwei Schritte und somit zwei Übungen vor uns:
1. **Metadaten über OAI-PMH harvesten mit VuFindHarvest:** Die Daten aus koha, ArchivesSpace und DSpace über die OAI-PMH-Schnittstellen harvesten und auf der Festplatte speichern.
2. **XSLT Crosswalks mit MarcEdit:** Mit MarcEdit die auf der Festplatte gespeicherten Daten alle ins Format MARC-21 bringen.


### Übung 1: Metadaten über OAI-PMH harvesten mit VuFindHarvest
Das Vorgehen ist wie folgt:
1. Sicherstellen, dass die Endpoints (koha, ArchivesSpace, DSpace) und die von uns im Vorfeld erstellten Datensätze überhaupt verfügbar sind und die Schnittstellen abrufbar
2. Installation von VuFindHarvest
3. Die Schnittstellen abrufen und die Daten als XML auf der Festplatte speichern.

Leider hat bereits die Installation von VuFindHarvest nicht funktioniert. Zunächst sah alles noch gut aus: 

![Screenshot Terminal Installation VuFindHarvest](https://pad.gwdg.de/uploads/upload_a8e037dfa1ac0f1f4b0a1f6d6a8874fd.png)

Als nächsten Befehl habe ich `composer install`eingegeben. Anschliessend passierte das folgende:

![Screenshot Fehler bei Installation VuFindHarvest](https://pad.gwdg.de/uploads/upload_253fa8ec582a1a6d31fccd7d296b768e.png)

Und das, befand ich, sah irgendwie nicht so gut aus, besonders, da das Terminal selbst eine Stelle extra gelb markiert hat. Ich habe dann dennoch einfach mal versucht, ob ich die Daten aus ArchiveSpaces bekommen kann – es hätte ja sein können, dass die Installation von VuFindHarvest doch geklappt hat und das normal ist, dass das so aussieht wie auf dem Screenshot oben. Allerdings hat das nicht funktioniert mit dem Laden der Daten. Ich weiss nun allerdings nicht, ob das an der fehlerhaften Installation von VuFindHarvest liegt, an einem fehlerhaften Vorgehen beim Zugrif auf ArchivesSpace oder eine Kombination aus allem.

Irgendetwas scheint es aufjedenfall schon gemacht zu haben, wie der folgende Screenshot der Ordner auf der virtuellen Maschine zeigt:

![Screenshot der bestehenden Ordner](https://pad.gwdg.de/uploads/upload_5913ddfb7999df2533b42e2c67b554ad.png)

Allerdings wusste ich dann damit nicht wirklich etwas anzufangen, und wie ich nun weitermachen sollte, fand ich ebensowenig heraus.

Hier kam ich nun also zum ersten Mal an den Punkt, wo ich alleine wirklich nicht mehr weiterkam. Hier wäre der Austausch in einer Gruppe sehr hilfreich gewesen. Hinzu kam der Umstand dass, da ich mit BAIN insgesamt derart in Rückstand geraten war, nun die Zeit so knapp wurde, dass es mir nicht mehr reichte, dem Problem nachzugehen und nachzufragen. Angesichts des Zeitdruckes habe ich entschieden, dass ich erstmal weitermache und versuche, die weiteren Schritte anhand der Vorlesungsaufzeichnung und des Skripts nachzuvollziehen. Wenn die Zeit dann doch noch reicht, dann kann ich dem Problem immer noch auf den Grund gehen, es hoffentlich lösen und die Aufgaben selbst nachvollziehen. Ungünstig ist natürlich, dass alles aufeinander aufbaut und ich vermutlich dadurch entsprechend bei den nachfolgenden Übungen anstehen werde, da mir nun die Datensätze fehlen.

